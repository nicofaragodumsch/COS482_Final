{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cc7edd-e002-4c14-9d76-82d76bdf4f39",
   "metadata": {},
   "source": [
    "# NASA Exoplanet Data Cleaning & Wrangling\n",
    "\n",
    "**Project:** K-Means Clustering Analysis of NASA Exoplanets \n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook demonstrates professional data wrangling practices:\n",
    "   \n",
    "1. **Identify and analyze missing values** in raw data\n",
    "2. **Filter to complete cases** for clustering analysis\n",
    "3. **Create derived variables** (density calculation)\n",
    "4. **Standardize text fields** for consistency\n",
    "5. **Detect and remove outliers** using statistical methods\n",
    "6. **Export clean dataset** for downstream analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Outcomes Demonstrated\n",
    "\n",
    "- âœ… Pandas data manipulation\n",
    "- âœ… Missing value analysis and handling\n",
    "- âœ… Data quality assessment\n",
    "- âœ… Outlier detection methods\n",
    "- âœ… Feature engineering\n",
    "- âœ… Documentation and reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2476aa-33df-40e3-8e07-d440f325f194",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151a3b6-f0b7-42bd-9c8a-63ecfb4c1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Notebook execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f3f4a-1cff-4318-8918-e585c29f9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "INPUT_FILE = 'raw_exoplanets.csv'\n",
    "OUTPUT_FILE = 'cleaned_exoplanets.csv'\n",
    "\n",
    "print(f\"Loading data from: {INPUT_FILE}\")\n",
    "df_raw = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded successfully!\")\n",
    "print(f\"  Shape: {df_raw.shape[0]:,} rows Ã— {df_raw.shape[1]} columns\")\n",
    "print(f\"\\nColumn names:\\n{df_raw.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c404d23-98b6-41c6-a38b-5145227c7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of raw data:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b6b3c-9756-4c7e-b5a0-c5fde38b8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875b02d-33d2-475a-97aa-abf61d37f9f3",
   "metadata": {},
   "source": [
    "## 2. Missing Value Analysis\n",
    "\n",
    "Before cleaning, we need to understand the extent and pattern of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65936885-09cb-4581-b533-1be45d4ab4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive missing value report\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    Generate detailed missing value analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe to analyze\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame : Missing value statistics\n",
    "    \"\"\"\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing_Count': df.isnull().sum().values,\n",
    "        'Missing_Percentage': (df.isnull().sum().values / len(df) * 100).round(2),\n",
    "        'Data_Type': df.dtypes.values,\n",
    "        'Non_Null_Count': df.notnull().sum().values\n",
    "    })\n",
    "    \n",
    "    # Sort by missing percentage\n",
    "    missing_stats = missing_stats.sort_values('Missing_Percentage', ascending=False)\n",
    "    missing_stats = missing_stats.reset_index(drop=True)\n",
    "    \n",
    "    return missing_stats\n",
    "\n",
    "missing_report = analyze_missing_values(df_raw)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(missing_report)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d51da2-3ea1-4f10-aa09-60f6eabdf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot of missing percentages\n",
    "missing_pct = (df_raw.isnull().sum() / len(df_raw) * 100).sort_values(ascending=False)\n",
    "ax1 = axes[0]\n",
    "missing_pct.plot(kind='barh', ax=ax1, color='coral')\n",
    "ax1.set_xlabel('Missing Percentage (%)')\n",
    "ax1.set_title('Missing Data by Column', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Heatmap of missing values (sample of data)\n",
    "ax2 = axes[1]\n",
    "sample_size = min(500, len(df_raw))\n",
    "sns.heatmap(df_raw.head(sample_size).isnull(), \n",
    "            cbar=True, \n",
    "            yticklabels=False,\n",
    "            cmap='RdYlGn_r',\n",
    "            ax=ax2)\n",
    "ax2.set_title(f'Missing Value Patterns (First {sample_size} Rows)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Columns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('missing_values_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Visualization saved as 'missing_values_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3d8e8-927f-4c78-b7a5-5707e7c17307",
   "metadata": {},
   "source": [
    "## 3. Filter to Complete Cases\n",
    "\n",
    "For clustering analysis, we need complete data for our key numerical features:\n",
    "- `pl_masse` (mass)\n",
    "- `pl_rade` (radius)\n",
    "- `pl_orbper` (orbital period)\n",
    "- `pl_eqt` (equilibrium temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43375f5-6553-4d78-88cb-c6f8575b6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define critical features for clustering\n",
    "CRITICAL_FEATURES = ['pl_masse', 'pl_rade', 'pl_orbper', 'pl_eqt']\n",
    "\n",
    "print(\"Critical features for clustering analysis:\")\n",
    "for i, feat in enumerate(CRITICAL_FEATURES, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# Check completeness before filtering\n",
    "print(f\"\\nBefore filtering:\")\n",
    "print(f\"  Total planets: {len(df_raw):,}\")\n",
    "for feat in CRITICAL_FEATURES:\n",
    "    missing = df_raw[feat].isnull().sum()\n",
    "    pct = (missing / len(df_raw) * 100)\n",
    "    print(f\"  {feat}: {missing:,} missing ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f74310-fef1-4df5-95a5-2372be08bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working copy and filter to complete cases\n",
    "df_clean = df_raw.copy()\n",
    "\n",
    "# Filter to rows with all critical features present\n",
    "mask_complete = df_clean[CRITICAL_FEATURES].notnull().all(axis=1)\n",
    "df_clean = df_clean[mask_complete].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering to complete cases:\")\n",
    "print(f\"  Total planets: {len(df_clean):,}\")\n",
    "print(f\"  Rows removed: {len(df_raw) - len(df_clean):,}\")\n",
    "print(f\"  Retention rate: {(len(df_clean) / len(df_raw) * 100):.2f}%\")\n",
    "\n",
    "# Verify no missing values in critical features\n",
    "print(f\"\\nVerification - Missing values in critical features:\")\n",
    "print(df_clean[CRITICAL_FEATURES].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35339f42-d28f-41ec-89c8-5e24a4c3082e",
   "metadata": {},
   "source": [
    "## 4. Create Derived Variables\n",
    "\n",
    "### Calculate Density\n",
    "\n",
    "Density is a crucial physical property for planet classification:\n",
    "\n",
    "$$\\\\text{Density} = \\\\frac{\\\\text{Mass}}{\\\\text{Volume}} = \\\\frac{\\\\text{Mass}}{\\\\frac{4}{3}\\\\pi r^3}$$\n",
    "\n",
    "For relative density (compared to Earth):\n",
    "$$\\\\text{Relative Density} = \\\\frac{\\\\text{Mass (Earth masses)}}{\\\\text{Radius}^3 \\\\text{ (Earth radii)}^3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873d502-2a0e-4c0d-87e7-fd34b93d75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate density (mass/radius^3)\n",
    "# This gives us relative density compared to Earth\n",
    "df_clean['density'] = df_clean['pl_masse'] / (df_clean['pl_rade'] ** 3)\n",
    "\n",
    "print(\"Density calculation complete!\\n\")\n",
    "print(\"Density statistics (relative to Earth):\")\n",
    "print(df_clean['density'].describe())\n",
    "\n",
    "# Reference values for context\n",
    "print(\"\\nðŸ“Š Reference densities (relative to Earth = 1.0):\")\n",
    "print(\"  â€¢ Gas giants (Jupiter): ~0.24\")\n",
    "print(\"  â€¢ Ice giants (Neptune): ~0.30\")\n",
    "print(\"  â€¢ Rocky planets (Earth): ~1.00\")\n",
    "print(\"  â€¢ Super-Earths: ~0.8-1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c13d00-21ff-4b7c-8505-6eeab9b3ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize density distribution with better handling of outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Remove extreme outliers for visualization (but keep in dataset for now)\n",
    "# Use reasonable range for visualization\n",
    "density_viz = df_clean['density'][(df_clean['density'] > 0.01) & (df_clean['density'] < 50)]\n",
    "\n",
    "# Histogram\n",
    "ax1 = axes[0]\n",
    "density_viz.hist(bins=50, edgecolor='black', alpha=0.7, ax=ax1)\n",
    "ax1.axvline(1.0, color='red', linestyle='--', linewidth=2, label='Earth density')\n",
    "ax1.set_xlabel('Density (relative to Earth)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Planet Densities\\n(Filtered to 0.01-50 for visualization)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Add statistics text\n",
    "mean_density = density_viz.mean()\n",
    "median_density = density_viz.median()\n",
    "ax1.text(0.98, 0.97, f'Mean: {mean_density:.2f}\\nMedian: {median_density:.2f}', \n",
    "         transform=ax1.transAxes, ha='right', va='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Log-scale histogram for full range\n",
    "ax2 = axes[1]\n",
    "df_clean['density'].hist(bins=50, edgecolor='black', alpha=0.7, ax=ax2)\n",
    "ax2.axvline(1.0, color='red', linestyle='--', linewidth=2, label='Earth density')\n",
    "ax2.set_xlabel('Density (relative to Earth)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Density Distribution (Log Scale - Full Range)', fontweight='bold')\n",
    "ax2.set_xscale('log')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('density_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization saved as 'density_distribution.png'\")\n",
    "print(f\"\\nFull dataset density range: {df_clean['density'].min():.2f} to {df_clean['density'].max():.2f}\")\n",
    "print(f\"Planets with density 0.01-50: {len(density_viz):,} ({len(density_viz)/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157faece-e4cf-4b72-af56-c590888f76f7",
   "metadata": {},
   "source": [
    "## 5. Standardize Text Fields\n",
    "\n",
    "Text standardization ensures consistency in categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ff493-9d0a-4ecb-a4ac-39795b1e1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine text fields before standardization\n",
    "print(\"Text fields before standardization:\\n\")\n",
    "\n",
    "print(\"1. Planet Names (pl_name) - Sample:\")\n",
    "print(df_clean['pl_name'].head(10).tolist())\n",
    "\n",
    "print(\"\\n2. Discovery Methods (discoverymethod) - Unique values:\")\n",
    "print(df_clean['discoverymethod'].value_counts())\n",
    "\n",
    "print(\"\\n3. Host Star Names (hostname) - Sample:\")\n",
    "print(df_clean['hostname'].head(10).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324f6e6-cf41-4037-9a3c-c923a53b70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text_fields(df):\n",
    "    \"\"\"\n",
    "    Standardize text fields for consistency.\n",
    "    \n",
    "    Operations:\n",
    "    - Strip whitespace\n",
    "    - Standardize discovery method names\n",
    "    - Ensure consistent capitalization for names\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to standardize\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame : Standardized DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Strip whitespace from all string columns\n",
    "    string_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "    \n",
    "    # Standardize discovery methods (capitalize consistently)\n",
    "    if 'discoverymethod' in df.columns:\n",
    "        df['discoverymethod'] = df['discoverymethod'].str.title()\n",
    "    \n",
    "    # Planet names: ensure no leading/trailing spaces\n",
    "    if 'pl_name' in df.columns:\n",
    "        df['pl_name'] = df['pl_name'].str.strip()\n",
    "    \n",
    "    # Host star names: ensure no leading/trailing spaces  \n",
    "    if 'hostname' in df.columns:\n",
    "        df['hostname'] = df['hostname'].str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply standardization\n",
    "df_clean = standardize_text_fields(df_clean)\n",
    "\n",
    "print(\"âœ“ Text fields standardized successfully!\\n\")\n",
    "print(\"Discovery Methods after standardization:\")\n",
    "print(df_clean['discoverymethod'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbcd737-2b66-4b46-9e0d-8d3cda48e035",
   "metadata": {},
   "source": [
    "We'll use multiple methods to detect outliers:\n",
    "1. **IQR Method** (Interquartile Range)\n",
    "2. **Z-Score Method** (Statistical)\n",
    "3. **Domain Knowledge** (Physical constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f0407-80c0-476d-9940-1bf567aeab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, column, multiplier=3.0):\n",
    "    \"\"\"\n",
    "    Detect outliers using the IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    column : str\n",
    "        Column name to check for outliers\n",
    "    multiplier : float\n",
    "        IQR multiplier (default 3.0 for extreme outliers)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pd.Series : Boolean mask (True = outlier)\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    \n",
    "    print(f\"\\n{column}:\")\n",
    "    print(f\"  Q1: {Q1:.4f}, Q3: {Q3:.4f}, IQR: {IQR:.4f}\")\n",
    "    print(f\"  Bounds: [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "    print(f\"  Outliers detected: {outliers.sum():,} ({outliers.sum()/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detect outliers in numerical features\n",
    "print(\"=\"*80)\n",
    "print(\"OUTLIER DETECTION (IQR Method, multiplier=3.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numerical_features = ['pl_masse', 'pl_rade', 'pl_orbper', 'pl_eqt', 'density']\n",
    "outlier_masks = {}\n",
    "\n",
    "for feature in numerical_features:\n",
    "    outlier_masks[feature] = detect_outliers_iqr(df_clean, feature, multiplier=3.0)\n",
    "\n",
    "# Combined outlier mask (any feature is an outlier)\n",
    "combined_outliers = pd.Series(False, index=df_clean.index)\n",
    "for mask in outlier_masks.values():\n",
    "    combined_outliers = combined_outliers | mask\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total rows with ANY outlier: {combined_outliers.sum():,} ({combined_outliers.sum()/len(df_clean)*100:.2f}%)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c43ca6-a40b-4669-81f8-0bc6d8952bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers before removal\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Box plot\n",
    "    df_clean.boxplot(column=feature, ax=ax, patch_artist=True)\n",
    "    ax.set_title(f'{feature}', fontweight='bold')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Add count of outliers\n",
    "    n_outliers = outlier_masks[feature].sum()\n",
    "    ax.text(0.5, 0.95, f'Outliers: {n_outliers}', \n",
    "            transform=ax.transAxes, \n",
    "            ha='center', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Outlier Detection: Box Plots of Numerical Features', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outliers_detection.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization saved as 'outliers_detection.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5f247-1dfa-436e-bc9b-1955d40e3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply domain knowledge filters\n",
    "print(\"\\nApplying domain knowledge filters:\\n\")\n",
    "\n",
    "# Physical constraints\n",
    "# 1. Density must be positive and reasonable (0.01 to 100 relative to Earth)\n",
    "valid_density = (df_clean['density'] > 0.01) & (df_clean['density'] < 100)\n",
    "print(f\"1. Valid density range (0.01 to 100): Removed {(~valid_density).sum():,} planets\")\n",
    "\n",
    "# 2. Mass and radius must be positive\n",
    "valid_mass_radius = (df_clean['pl_masse'] > 0) & (df_clean['pl_rade'] > 0)\n",
    "print(f\"2. Positive mass and radius: Removed {(~valid_mass_radius).sum():,} planets\")\n",
    "\n",
    "# 3. Orbital period must be positive\n",
    "valid_period = df_clean['pl_orbper'] > 0\n",
    "print(f\"3. Positive orbital period: Removed {(~valid_period).sum():,} planets\")\n",
    "\n",
    "# 4. Temperature must be positive and below stellar surface temp (~50,000 K max)\n",
    "valid_temp = (df_clean['pl_eqt'] > 0) & (df_clean['pl_eqt'] < 5000)\n",
    "print(f\"4. Reasonable temperature (0-5000 K): Removed {(~valid_temp).sum():,} planets\")\n",
    "\n",
    "# Combine all validity checks\n",
    "valid_data = valid_density & valid_mass_radius & valid_period & valid_temp\n",
    "print(f\"\\nTotal planets failing domain checks: {(~valid_data).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e1247-4549-4e23-ae6c-f2c23ab50911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRemoving outliers and invalid data...\\n\")\n",
    "print(f\"Before outlier removal: {len(df_clean):,} planets\")\n",
    "\n",
    "# Keep only valid data (non-outliers based on domain knowledge)\n",
    "df_clean = df_clean[valid_data].copy()\n",
    "\n",
    "# Additionally remove extreme statistical outliers (using IQR method)\n",
    "# We'll be more conservative and only remove the most extreme outliers\n",
    "df_clean = df_clean[~combined_outliers].copy()\n",
    "\n",
    "print(f\"After outlier removal: {len(df_clean):,} planets\")\n",
    "print(f\"Planets removed: {len(df_raw) - len(df_clean):,}\")\n",
    "print(f\"Retention rate: {(len(df_clean) / len(df_raw) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92074966-714b-4a3c-bd0c-d0cc93450419",
   "metadata": {},
   "source": [
    "## 7. Final Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9c66f-bf03-4c2b-9b4e-241ef44635f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL CLEANED DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nShape: {df_clean.shape[0]:,} rows Ã— {df_clean.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nNumerical Features - Descriptive Statistics:\")\n",
    "print(df_clean[numerical_features].describe())\n",
    "\n",
    "print(\"\\nCategorical Features:\")\n",
    "print(f\"  â€¢ Unique discovery methods: {df_clean['discoverymethod'].nunique()}\")\n",
    "print(f\"  â€¢ Unique host stars: {df_clean['hostname'].nunique()}\")\n",
    "print(f\"  â€¢ Discovery year range: {df_clean['disc_year'].min():.0f} - {df_clean['disc_year'].max():.0f}\")\n",
    "\n",
    "print(\"\\nMissing values in critical features:\")\n",
    "print(df_clean[CRITICAL_FEATURES + ['density']].isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebd1da-7b7f-47c1-aed9-ac5c7c0c8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Discovery methods distribution\n",
    "ax1 = axes[0, 0]\n",
    "method_counts = df_clean['discoverymethod'].value_counts().head(10)\n",
    "method_counts.plot(kind='barh', ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Count')\n",
    "ax1.set_title('Top 10 Discovery Methods', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Discovery year distribution\n",
    "ax2 = axes[0, 1]\n",
    "df_clean['disc_year'].hist(bins=30, ax=ax2, color='lightgreen', edgecolor='black')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.set_ylabel('Number of Discoveries')\n",
    "ax2.set_title('Exoplanet Discoveries Over Time', fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Mass vs Radius (cleaned data)\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(df_clean['pl_masse'], df_clean['pl_rade'], \n",
    "                     alpha=0.5, s=20, c=df_clean['density'], cmap='viridis')\n",
    "ax3.set_xlabel('Mass (Earth masses)')\n",
    "ax3.set_ylabel('Radius (Earth radii)')\n",
    "ax3.set_title('Mass vs Radius (colored by density)', fontweight='bold')\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "plt.colorbar(scatter, ax=ax3, label='Density')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Orbital period vs temperature\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(df_clean['pl_orbper'], df_clean['pl_eqt'], alpha=0.5, s=20, color='coral')\n",
    "ax4.set_xlabel('Orbital Period (days)')\n",
    "ax4.set_ylabel('Equilibrium Temperature (K)')\n",
    "ax4.set_title('Orbital Period vs Temperature', fontweight='bold')\n",
    "ax4.set_xscale('log')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Cleaned Dataset Overview', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cleaned_data_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization saved as 'cleaned_data_summary.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa775dc5-564f-4c8a-8f1d-d6fb250416d5",
   "metadata": {},
   "source": [
    "## 8. Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1434f84-0f25-4807-bd4f-2cb0db80aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Exporting cleaned data to: {OUTPUT_FILE}\\n\")\n",
    "\n",
    "df_clean.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "import os\n",
    "file_size = os.path.getsize(OUTPUT_FILE)\n",
    "\n",
    "print(\"âœ“ Export successful!\")\n",
    "print(f\"  File: {OUTPUT_FILE}\")\n",
    "print(f\"  Size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")\n",
    "print(f\"  Rows: {len(df_clean):,}\")\n",
    "print(f\"  Columns: {len(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901fad0-60ab-4f20-bdf9-cb84faec4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data cleaning report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "DATA CLEANING REPORT\n",
    "{'='*80}\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "INPUT DATA:\n",
    "  â€¢ File: {INPUT_FILE}\n",
    "  â€¢ Initial rows: {len(df_raw):,}\n",
    "  â€¢ Initial columns: {len(df_raw.columns)}\n",
    "\n",
    "CLEANING OPERATIONS:\n",
    "  1. Filtered to complete cases for critical features\n",
    "  2. Created derived variable: density = mass/radiusÂ³\n",
    "  3. Standardized text fields (discovery methods, names)\n",
    "  4. Removed outliers using IQR method (multiplier=3.0)\n",
    "  5. Applied domain knowledge filters\n",
    "\n",
    "OUTPUT DATA:\n",
    "  â€¢ File: {OUTPUT_FILE}\n",
    "  â€¢ Final rows: {len(df_clean):,}\n",
    "  â€¢ Final columns: {len(df_clean.columns)}\n",
    "  â€¢ Retention rate: {(len(df_clean) / len(df_raw) * 100):.2f}%\n",
    "\n",
    "CRITICAL FEATURES (Complete, no missing values):\n",
    "  â€¢ pl_masse (mass)\n",
    "  â€¢ pl_rade (radius)\n",
    "  â€¢ pl_orbper (orbital period)\n",
    "  â€¢ pl_eqt (equilibrium temperature)\n",
    "  â€¢ density (derived variable)\n",
    "\n",
    "DATA QUALITY:\n",
    "  â€¢ No missing values in critical features: âœ“\n",
    "  â€¢ Outliers removed: âœ“\n",
    "  â€¢ Text fields standardized: âœ“\n",
    "  â€¢ Domain constraints validated: âœ“\n",
    "\n",
    "READY FOR:\n",
    "  âœ“ Exploratory Data Analysis\n",
    "  âœ“ Database implementation\n",
    "  âœ“ K-Means clustering analysis\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report to file\n",
    "with open('data_cleaning_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\nâœ“ Report saved as 'data_cleaning_report.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74a4fdb-83ea-4480-9e7e-8d596acc86db",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. âœ… **Identified missing values** - Comprehensive analysis of data completeness\n",
    "2. âœ… **Filtered to complete cases** - Retained only planets with all critical features\n",
    "3. âœ… **Created derived variable** - Calculated density for each planet\n",
    "4. âœ… **Standardized text fields** - Ensured consistency in categorical data\n",
    "5. âœ… **Removed outliers** - Applied both statistical and domain knowledge filters\n",
    "6. âœ… **Exported clean data** - Saved `cleaned_exoplanets.csv` for next phase\n",
    "\n",
    "### Data Quality Summary:\n",
    "\n",
    "- **No missing values** in critical clustering features\n",
    "- **Physically valid** data (positive values, reasonable ranges)\n",
    "- **Standardized** text fields for consistency\n",
    "- **Outliers removed** using rigorous statistical methods\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Quick EDA** - Create initial visualizations\n",
    "2. **Database Design** - Design ER diagram and PostgreSQL schema\n",
    "3. **K-Means Clustering**  - Perform clustering analysis on cleaned data\n",
    "\n",
    "---\n",
    "\n",
    "**Files Generated:**\n",
    "- `cleaned_exoplanets.csv` - Clean dataset ready for analysis\n",
    "- `missing_values_analysis.png` - Missing data visualization\n",
    "- `density_distribution.png` - Density analysis\n",
    "- `outliers_detection.png` - Outlier detection plots\n",
    "- `cleaned_data_summary.png` - Final data overview\n",
    "- `data_cleaning_report.txt` - Comprehensive cleaning report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
